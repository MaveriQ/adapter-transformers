{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e732d4-2a04-4620-9c0c-5ffcf6a12759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LiltForPretraining, HarrysConfig, HarrysInvConfig\n",
    "from pl_lilt_datamodule import LiltPretrainingDataModule\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf359893-0aec-4cb6-b727-9ea01bbf4fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rvl_cdip (/home/hj36wegi/scratch/data/huggingface/datasets/rvl_cdip/default/1.0.0/ea410993ed3f5b9744d8616ffbaad5f70a75a21a4233626dd07b3de31d381e53)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544c2ee4630347a1a7faa8c4f8cf27f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 777] at entry 0 and [1000, 769] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m dm\u001b[39m.\u001b[39msetup()\n\u001b[1;32m      8\u001b[0m train_loader \u001b[39m=\u001b[39m dm\u001b[39m.\u001b[39mtrain_dataloader()\n\u001b[0;32m----> 9\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))\n",
      "File \u001b[0;32m~/.conda/envs/adapters/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/adapters/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/adapters/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/.conda/envs/adapters/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m/work/home/hj36wegi/git/adapter-transformers/pl_model/data_collator.py:70\u001b[0m, in \u001b[0;36mDataCollatorForLiltPretraining.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, examples: List[Union[List[\u001b[39mint\u001b[39m], Any, Dict[\u001b[39mstr\u001b[39m, Any]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(examples)\n\u001b[0;32m---> 70\u001b[0m     batch \u001b[39m=\u001b[39m {k:torch\u001b[39m.\u001b[39mstack([ex[k] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m examples],dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m examples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     72\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_bounding_boxes(batch)\n\u001b[1;32m     73\u001b[0m     \u001b[39m# If special token mask has been preprocessed, pop it from the dict.\u001b[39;00m\n",
      "File \u001b[0;32m/work/home/hj36wegi/git/adapter-transformers/pl_model/data_collator.py:70\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, examples: List[Union[List[\u001b[39mint\u001b[39m], Any, Dict[\u001b[39mstr\u001b[39m, Any]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(examples)\n\u001b[0;32m---> 70\u001b[0m     batch \u001b[39m=\u001b[39m {k:torch\u001b[39m.\u001b[39;49mstack([ex[k] \u001b[39mfor\u001b[39;49;00m ex \u001b[39min\u001b[39;49;00m examples],dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m examples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     72\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_bounding_boxes(batch)\n\u001b[1;32m     73\u001b[0m     \u001b[39m# If special token mask has been preprocessed, pop it from the dict.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 777] at entry 0 and [1000, 769] at entry 1"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser = LiltPretrainingDataModule.add_model_specific_args(parser)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "dm = LiltPretrainingDataModule(args)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e64499-ecfc-46fa-a8f4-40c1f387ee18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'bbox', 'bbox_loc', 'input_labels', 'bbox_labels'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55f5c19-60e8-4920-adb0-f4749f175c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_adapter_config = HarrysInvConfig(text_output_adapter=True, layout_output_adapter=False)\n",
    "layout_adapter_config = HarrysConfig(text_output_adapter=False, layout_output_adapter=True)\n",
    "task_adapter_config = HarrysConfig(text_output_adapter=True, layout_output_adapter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e92a5f-964c-4f5a-b827-e0bf6feab480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LiltForPretraining were not initialized from the model checkpoint at nielsr/lilt-xlm-roberta-base and are newly initialized: ['lm_head.kpl_head.layer_norm.bias', 'lm_head.mvlm_head.dense.weight', 'lm_head.mvlm_head.layer_norm.bias', 'lm_head.cai_head.decoder.weight', 'lm_head.cai_head.dense.bias', 'lm_head.cai_head.layer_norm.weight', 'lm_head.cai_head.decoder.bias', 'lm_head.kpl_head.layer_norm.weight', 'lm_head.mvlm_head.layer_norm.weight', 'lm_head.cai_head.dense.weight', 'lm_head.kpl_head.dense.weight', 'lm_head.kpl_head.decoder.weight', 'lm_head.mvlm_head.bias', 'lm_head.mvlm_head.dense.bias', 'lm_head.kpl_head.bias', 'lm_head.kpl_head.decoder.bias', 'lm_head.kpl_head.dense.bias', 'lm_head.cai_head.bias', 'lm_head.cai_head.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LiltForPretraining.from_pretrained(\"nielsr/lilt-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4f1095-1065-4215-9448-e3d735d7e4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add_adapter(\"layout\", layout_adapter_config)\n",
    "model.add_adapter(\"language\", lang_adapter_config)\n",
    "model.add_adapter(\"task\", task_adapter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ca5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madapter_setup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdapterCompositionBlock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Sets the model into mode for training the given adapters. If self.base_model is self, must inherit from a class\n",
      "that implements this method, to preclude infinite recursion\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/adapters/lib/python3.10/site-packages/transformers/adapters/model_mixin.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.train_adapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16273e19-dbe3-44b6-bd25-0a731a129f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amodel.train_adapter([\"layout\",\"task\", \"language\"])\n",
    "# model.freeze_model(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6630c366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiltLayer(\n",
       "  (attention): LiltAttention(\n",
       "    (self): LiltSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layout_query): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (layout_key): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (layout_value): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): LiltSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (adapters): ModuleDict()\n",
       "      (adapter_fusion_layer): ModuleDict()\n",
       "    )\n",
       "    (layout_output): LiltSelfOutput(\n",
       "      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (LayerNorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (adapters): ModuleDict()\n",
       "      (adapter_fusion_layer): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (intermediate): LiltIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): LiltOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (adapters): ModuleDict(\n",
       "      (language): Adapter(\n",
       "        (non_linearity): Activation_Function_Class(\n",
       "          (f): ReLU()\n",
       "        )\n",
       "        (adapter_down): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): Activation_Function_Class(\n",
       "            (f): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "      )\n",
       "      (task): Adapter(\n",
       "        (non_linearity): Activation_Function_Class(\n",
       "          (f): ReLU()\n",
       "        )\n",
       "        (adapter_down): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "          (1): Activation_Function_Class(\n",
       "            (f): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (adapter_fusion_layer): ModuleDict()\n",
       "  )\n",
       "  (layout_intermediate): LiltIntermediate(\n",
       "    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (layout_output): LiltOutput(\n",
       "    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "    (LayerNorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (adapters): ModuleDict(\n",
       "      (layout): Adapter(\n",
       "        (non_linearity): Activation_Function_Class(\n",
       "          (f): ReLU()\n",
       "        )\n",
       "        (adapter_down): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
       "          (1): Activation_Function_Class(\n",
       "            (f): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (adapter_up): Linear(in_features=48, out_features=192, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (adapter_fusion_layer): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lilt.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0d5d09-2b37-450d-9f33-714f8c9e8c29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n",
      "language\n",
      "task\n",
      "layout\n"
     ]
    }
   ],
   "source": [
    "out = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34822d93-a6d4-42d4-b4c4-95cd761d5ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ output.adapters.language.adapter_down.0.weight\n",
      "+ output.adapters.language.adapter_down.0.bias\n",
      "+ output.adapters.language.adapter_up.weight\n",
      "+ output.adapters.language.adapter_up.bias\n",
      "+ output.adapters.task.adapter_down.0.weight\n",
      "+ output.adapters.task.adapter_down.0.bias\n",
      "+ output.adapters.task.adapter_up.weight\n",
      "+ output.adapters.task.adapter_up.bias\n",
      "+ layout_output.adapters.layout.adapter_down.0.weight\n",
      "+ layout_output.adapters.layout.adapter_down.0.bias\n",
      "+ layout_output.adapters.layout.adapter_up.weight\n",
      "+ layout_output.adapters.layout.adapter_up.bias\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.lilt.encoder.layer[0].named_parameters():\n",
    "    if 'adapter' in n:\n",
    "        if p.requires_grad:\n",
    "            print(\"+ \" + n)\n",
    "        else:\n",
    "            print(\"- \" + n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bb177d3-df97-4d8f-bcfc-26a745295f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ad303-ed75-4b34-8a01-a11e813daac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
